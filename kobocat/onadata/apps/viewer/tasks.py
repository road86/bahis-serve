import re
import os
import sys
from celery import task
from celery import shared_task
from django.db import transaction
from django.conf import settings
from django.core.mail import mail_admins
from requests import ConnectionError
import requests

import string
import openpyxl
from openpyxl import Workbook
from openpyxl.cell import get_column_letter

from pyxform.builder import create_survey_from_xls
#from pyxform.xls2json import SurveyReader
from pyxform.xls2json_backends import xls_to_dict, csv_to_dict
import pyxform.constants as constants
import pyxform.aliases as aliases


from onadata.apps.viewer.models.export import Export

from onadata.apps.bh_module.utility_functions import datasource_query_generate

from onadata.libs.exceptions import NoRecordsFoundError
from onadata.libs.utils.export_tools import generate_export, \
    generate_attachments_zip_export, generate_kml_export, \
    generate_external_export
from onadata.libs.utils.logger_tools import mongo_sync_status, report_exception

from pymongo import MongoClient
import pandas as pd
import psycopg2
import json
from pandas import ExcelWriter
from copy import deepcopy
from datetime import datetime
from django.db import connection
import os.path


def create_db_async_export(xform, export_type, query, force_xlsx=None, options=None):
    username = xform.user.username
    id_string = xform.id_string
    print "#############################--Test-1"

    @transaction.commit_on_success
    def _create_export(xform, export_type):
        return Export.objects.create(xform=xform, export_type=export_type)

    export = _create_export(xform, export_type)
    result = None
    arguments = {
        'username': username,
        'id_string': id_string,
        'export_id': export.id,
        'query': query,
    }
    # print export_type
    if export_type in [Export.XLS_EXPORT, Export.GDOC_EXPORT,
                       Export.CSV_EXPORT, Export.CSV_ZIP_EXPORT,
                       Export.SAV_ZIP_EXPORT, Export.DB_EXPORT]:
        if options and "group_delimiter" in options:
            arguments["group_delimiter"] = options["group_delimiter"]
        if options and "split_select_multiples" in options:
            arguments["split_select_multiples"] = \
                options["split_select_multiples"]
        if options and "binary_select_multiples" in options:
            arguments["binary_select_multiples"] = \
                options["binary_select_multiples"]

        if options and "exp_data_typ" in options and export_type == Export.XLS_EXPORT:
            dt_type = options["exp_data_typ"]
            if dt_type == 'lbl':
                arguments["show_label"] = True
            else:
                arguments["show_label"] = False

        # start async export
        if export_type in [Export.XLS_EXPORT, Export.GDOC_EXPORT]:
            result = create_xls_export.apply_async((), arguments, countdown=10)
        elif export_type == Export.DB_EXPORT:
            result = create_db_export.apply_async((), arguments, countdown=10)
        elif export_type == Export.CSV_EXPORT:
            result = create_csv_export.apply_async(
                (), arguments, countdown=10)
        elif export_type == Export.CSV_ZIP_EXPORT:
            result = create_csv_zip_export.apply_async(
                (), arguments, countdown=10)
        elif export_type == Export.SAV_ZIP_EXPORT:
            result = create_sav_zip_export.apply_async(
                (), arguments, countdown=10)
        else:
            raise Export.ExportTypeError
    elif export_type == Export.ZIP_EXPORT:
        # start async export
        result = create_zip_export.apply_async(
            (), arguments, countdown=10)
    elif export_type == Export.KML_EXPORT:
        # start async export
        result = create_kml_export.apply_async(
            (), arguments, countdown=10)
    elif export_type == Export.EXTERNAL_EXPORT:
        if options and "token" in options:
            arguments["token"] = options["token"]
        if options and "meta" in options:
            arguments["meta"] = options["meta"]

        result = create_external_export.apply_async(
            (), arguments, countdown=10)
    else:
        raise Export.ExportTypeError
    if result:
        # when celery is running eager, the export has been generated by the
        # time we get here so lets retrieve the export object a fresh before we
        # save
        if settings.CELERY_ALWAYS_EAGER:
            export = Export.objects.get(id=export.id)
        export.task_id = result.task_id
        export.save()
        return export, result
    return None


def create_async_export(xform, export_type, query, force_xlsx, options=None):
    print("Export 4______________________________________________________________________________________")
    username = xform.user.username
    id_string = xform.id_string

    @transaction.commit_on_success
    def _create_export(xform, export_type):
        return Export.objects.create(xform=xform, export_type=export_type)

    export = _create_export(xform, export_type)
    result = None
    arguments = {
        'username': username,
        'id_string': id_string,
        'export_id': export.id,
        'query': query,
    }
    if export_type in [Export.XLS_EXPORT, Export.GDOC_EXPORT,
                       Export.CSV_EXPORT, Export.CSV_ZIP_EXPORT,
                       Export.SAV_ZIP_EXPORT]:
        if options and "group_delimiter" in options:
            arguments["group_delimiter"] = options["group_delimiter"]
        if options and "split_select_multiples" in options:
            arguments["split_select_multiples"] = \
                options["split_select_multiples"]
        if options and "binary_select_multiples" in options:
            arguments["binary_select_multiples"] = \
                options["binary_select_multiples"]
        if options and "exp_data_typ" in options and export_type == Export.XLS_EXPORT:
            # if options and "exp_data_typ" in options:
            dt_type = options["exp_data_typ"]
            if dt_type == 'lbl':
                arguments["show_label"] = True
            else:
                arguments["show_label"] = False
        if options and "export_label_lang" in options and dt_type == 'lbl':
            arguments["label_lang"] = options["export_label_lang"]
        if options and "default_xls_fields" in options:
            arguments["default_xls_fields"] = options["default_xls_fields"]

        # Need to pass export table header type label/key. in arguments 
        # from templates. ---shiam.

        # start async export
        if export_type in [Export.XLS_EXPORT, Export.GDOC_EXPORT]:
            result = create_xls_export.apply_async((), arguments, countdown=10)
        elif export_type == Export.CSV_EXPORT:
            result = create_csv_export.apply_async(
                (), arguments, countdown=10)
        elif export_type == Export.CSV_ZIP_EXPORT:
            result = create_csv_zip_export.apply_async(
                (), arguments, countdown=10)
        elif export_type == Export.SAV_ZIP_EXPORT:
            result = create_sav_zip_export.apply_async(
                (), arguments, countdown=10)
        else:
            raise Export.ExportTypeError
    elif export_type == Export.ZIP_EXPORT:
        # start async export
        result = create_zip_export.apply_async(
            (), arguments, countdown=10)
    elif export_type == Export.KML_EXPORT:
        # start async export
        result = create_kml_export.apply_async(
            (), arguments, countdown=10)
    elif export_type == Export.EXTERNAL_EXPORT:
        if options and "token" in options:
            arguments["token"] = options["token"]
        if options and "meta" in options:
            arguments["meta"] = options["meta"]

        result = create_external_export.apply_async(
            (), arguments, countdown=10)
    else:
        raise Export.ExportTypeError
    if result:
        # when celery is running eager, the export has been generated by the
        # time we get here so lets retrieve the export object a fresh before we
        # save
        if settings.CELERY_ALWAYS_EAGER:
            export = Export.objects.get(id=export.id)
        export.task_id = result.task_id
        export.save()
        return export, result
    return None


def new_create_async_export(lan, xform, query, edt, id_string, export_type):
    languages = []
    print "here in new create"
    #@transaction.commit_on_success
    @transaction.atomic
    def _create_export(xform, export_type):
        return Export.objects.create(xform=xform, export_type=export_type)

    export = _create_export(xform, export_type)
    #print export.id
    show_label=False
    username = xform.user.username
    arguments = {
        'username': username,
        'id_string': id_string,
        'export_id': export.id,
        'query': query,
        'xform_id': xform.id,
        'show_label': show_label
    }
    print arguments
    #create_custom_export(username,id_string,show_label,xform.id,export.id,query)
    result = create_custom_export.apply_async((), arguments, countdown=10)
    result=False
    print result
    if result:
        # when celery is running eager, the export has been generated by the
        # time we get here so lets retrieve the export object a fresh before we
        # save
        if settings.CELERY_ALWAYS_EAGER:
            export = Export.objects.get(id=export.id)
        export.task_id = result.task_id
        export.save()
        return export, result
    else:
        export.task_id = "test task"
        export.save()
        return export, result
        
    return None


@shared_task
def create_custom_export(username, id_string, show_label, xform_id, export_id, query=None,
                           token=None, meta=None):
    print "here"+str(export_id)
    export = Export.objects.get(id=export_id)
    print "Before Query"
    print(query)
    #export_data(xform_id, show_label, query, export_id, username)
    try:
        # though export is not available when for has 0 submissions, we
        # catch this since it potentially stops celery
        print "Before Query"
        print (query)
        #export_data(xform_id, show_label, query, export_id, username)
        #get_data(xform_id)
        get_data(xform_id,id_string, show_label, query, export_id, username)
        
    
    except (Exception, NoRecordsFoundError, ConnectionError) as e:
        print(e)
        export.internal_status = Export.FAILED
        export.save()
        # mail admins
        details = {
            'export_id': export_id,
            'username': username,
            'id_string': id_string
        }
        report_exception("External Export Exception: Export ID - "
                         "%(export_id)s, /%(username)s/%(id_string)s"
                         % details, e, sys.exc_info())
        raise    

def get_csv_map(df):
    
    #df=df_field[["field_name","cntrl"]]
    
    fileds = list(df["field_name"])
    fileds= list(set(fileds))
    csv_map={}
    for l in fileds:
        cntrl = str(df[df["field_name"]==l].iloc[0]["cntrl"])
        x = re.findall("search[ ]*[(][' ,{}$a-zA-Z_0-9]*[)]", cntrl)        
        search = x[0] if len(x)>0 else ''
        if search != "":
            s=search.split('(')
            s=s[1] if len(s)>1 else ''
            s=s.split(',')
            s=s[0] if len(s)>0 else ''
            s=s.split('\'')
            s=s[1] if len(s)>1 else ''            
            csv_map[l]=s


    return csv_map

def db_reader(cr):
    for row in cr.fetchall():
        yield row  


def create_table_script(form_elements, form_id_string, **kwargs):
    """
        Recursive function for generating flat table scripts

        Parameters
        ----------
        form_elements : `list`
            List of form elements
        form_id_string : `str`,
            Form id string

        **kwargs Parameters
        ----------------
        parent : `str`, optional
            parent table name for establishing the link
    """
    table_cols = []
    for form_element in form_elements:
        if form_element['type'] != 'repeat' and form_element['type'] != 'group':
            if form_element['name'] != 'end' and form_element['name'] != 'start':
                table_cols.append({((kwargs['repeat'] + "/" + form_element['name']) if kwargs['repeat'] !="" else form_element['name']):form_element['name']})
                pass
        else:
            if form_element['type'] == 'repeat':
                repeat_element= (kwargs['repeat'] + "/" + form_element['name']) if kwargs['repeat'] !="" else form_element['name']
                create_table_script(form_element['children'], form_id_string + '_' + form_element['name'],
                                    parent=form_id_string + '_id',repeat_table=kwargs['repeat_table'],repeat=repeat_element)
            elif form_element['type'] == 'group':
                for fc in form_element['children']:
                    if fc['type'] != 'repeat':
                        #table_cols.append(form_element['name'] + '_' + fc['name'])
                        table_cols.append({((kwargs['repeat'] + "/" + form_element['name']+ '/' + fc['name']) if kwargs['repeat'] !="" else form_element['name']+ '/' + fc['name']):form_element['name'] + '_' + fc['name']})
                    else:
                        repeat_element= (kwargs['repeat'] + "/" + form_element['name'] + "/" + fc['name']) if kwargs['repeat'] !="" else (form_element['name'] + "/" + fc['name'])
                        create_table_script(fc['children'], form_id_string + '_' + fc['name'],
                                            parent=form_id_string + '_id',repeat_table=kwargs['repeat_table'],repeat=repeat_element)

    if kwargs.has_key('parent'):
        table_cols.append({kwargs['parent']:kwargs['parent']})
    table_cols.append({'instanceid':'instanceid'})
    table_cols.append({'xform_id':'xform_id'})
    kwargs['repeat_table'].append({"table_name":form_id_string,"repeat":kwargs['repeat'],"table_cols":table_cols})
    #print table_cols
    #print(form_id_string,kwargs['repeat'])
    #generate_table_schema(table_cols, form_id_string)

'''
def datasource_query_generate(data_source_id):
    if str(data_source_id)=="91":
        qry="""with p as (select "parent","loc_type","name","value" from core.geo_cluster ), s as (with p as (select "parent","loc_type","name","value" from core.geo_cluster ), s as (with p as (select "parent","loc_type","name","value" from core.geo_cluster ), s as (with p as (select "name","parent","value","longitude","latitude","loc_type","id" from core.geo_cluster ), s as (with p as (select "name","parent","value","longitude","latitude","loc_type","id" from core.geo_cluster ) select p.name as "division_name", p.parent, p.value as "division_code", p.longitude, p.latitude, p.loc_type, p.id from p  where ( cast(p.loc_type as numeric)  =  cast('1' as numeric) ))select p.name as "district_name", p.parent, p.value as "district_code", p.longitude, p.latitude, p.loc_type, p.id,s.division_name, s.division_code from p  Inner Join s on cast(p.parent as text) = cast(s.division_code as text) where ( cast(p.loc_type as numeric)  =  cast('2' as numeric) ))select p.loc_type, p.value as "upazila_code", p.name as "upazila_name", p.parent as "upazila_parent",s.division_name, s.district_name, s.division_code, s.district_code from p  Inner Join s on cast(p.parent as text) = cast(s.district_code as text) where ( cast(p.loc_type as numeric)  =  cast('3' as numeric) ))select p.loc_type as "union_loc", p.value as "union_code", p.name as "union_name", p.parent as "union_parent",s.division_name, s.upazila_code, s.upazila_name, s.division_code, s.district_code, s.district_name from p  Inner Join s on cast(p.parent as text) = cast(s.upazila_code as text) where ( cast(p.loc_type as numeric)  =  cast('4' as numeric) ))select p.loc_type as "mouza_loc_type", p.value as "mouza_code", p.name as "mouza_name", p.parent as "mouza_parent",s.division_name, s.union_code, s.upazila_code, s.upazila_name, s.district_code, s.division_code, s.union_name, s.district_name from p  Inner Join s on cast(p.parent as text) = cast(s.union_code as text) where ( cast(p.loc_type as numeric)  =  cast('5' as numeric) )"""
    else:
        qry="""with p as (select "parent","loc_type","name","value" from core.geo_cluster ), s as (with p as (select "parent","loc_type","name","value" from core.geo_cluster ), s as (with p as (select "name","parent","value","longitude","latitude","loc_type","id" from core.geo_cluster ), s as (with p as (select "name","parent","value","longitude","latitude","loc_type","id" from core.geo_cluster ) select p.name as "division_name", p.parent, p.value as "division_code", p.longitude, p.latitude, p.loc_type, p.id from p  where ( cast(p.loc_type as numeric)  =  cast('1' as numeric) ))select p.name as "district_name", p.parent, p.value as "district_code", p.longitude, p.latitude, p.loc_type, p.id,s.division_name, s.division_code from p  Inner Join s on cast(p.parent as text) = cast(s.division_code as text) where ( cast(p.loc_type as numeric)  =  cast('2' as numeric) ))select p.loc_type, p.value as "upazila_code", p.name as "upazila_name", p.parent as "upazila_parent",s.division_name, s.district_name, s.division_code, s.district_code from p  Inner Join s on cast(p.parent as text) = cast(s.district_code as text) where ( cast(p.loc_type as numeric)  =  cast('3' as numeric) ))select p.loc_type as "union_loc", p.value as "union_code", p.name as "union_name", p.parent as "union_parent",s.division_name, s.upazila_code, s.upazila_name, s.division_code, s.district_code, s.district_name from p  Inner Join s on cast(p.parent as text) = cast(s.upazila_code as text) where ( cast(p.loc_type as numeric)  =  cast('4' as numeric) )"""
    return qry
'''

def get_datasource_data_from_url(qry):
    web_server = settings.WEB_SERVER
    url = str(web_server)+"/bhmodule/get/query-result/"
    files=[
    ]
    headers = {}
    payload={'query':qry}
    try:
        response = requests.request("POST", url, headers=headers, data=payload, files=files)
        json_data= json.loads(response.text)
        df = pd.DataFrame(json_data)
        return df
    except:
        return pd.DataFrame()

def get_data(xform_id,id_string, show_label, qry_condition, export_id, username):
    print xform_id,id_string, show_label, qry_condition, export_id, username
    file_dir = 'onadata/media/' + username + "/exports/" + id_string + "/xls"

    if not os.path.exists(file_dir):
        os.makedirs(file_dir)
    time_now =  datetime.now().strftime(
        "%Y_%m_%d_%H_%M_%S") 
    f_name = id_string + '_' + time_now + ".xlsx"
    file_name = 'onadata/media/' + username + '/exports/' + id_string+ '/xls/' + f_name
    wb = Workbook()
    ws = wb.active
    

    qry="select id_string,json from instance.logger_xform where id = %s;" %(xform_id)
    form_data = pd.read_sql(qry, connection)
    form_def = json.loads(form_data.iloc[0]['json'])
    form_id_string = form_data.iloc[0]['id_string']
    form_elements = form_def['children']

    ws.title = form_id_string[:30]

    rpt_table = []
    create_table_script(form_elements, form_id_string,repeat_table=rpt_table,repeat="")
    #print "#####################"
    #print(rpt_table)    
    print "#####################"

    qry="select field_name,field_type ,value_text,value_label,choice_filter,cntrl from instance.xform_extracted where xform_id = "+str(xform_id)

    df_field_list = pd.read_sql(qry,connection)

    field_ss = list(df_field_list[df_field_list["field_type"]=="select one"]["field_name"])
    field_ss = list(set(field_ss))

    field_ms = list(df_field_list[df_field_list["field_type"]=="select all that apply"]["field_name"])
    field_ms = list(set(field_ms))    




    rpt_list = pd.read_sql("select field_name,sl_no from instance.xform_extracted where xform_id ="+ str(xform_id) +" and field_type ='repeat' order by sl_no",connection)

    
    #rows = filter(lambda sl: l == '' , rpt_table)
    #writer = pd.ExcelWriter('pandas_multiple.xlsx', engine='xlsxwriter')

    csv_map = get_csv_map(df_field_list)

    #print csv_map

    

    qry="select coalesce(csv_config,'') csv_config from core.xform_config_data where xform_id = "+str(xform_id)
    df_data_source = pd.read_sql(qry,connection)

    csv_data={}

    if len(df_data_source)>0:
        csv_config = df_data_source.iloc[0]["csv_config"]
        if csv_config !="":
            csv_config = json.loads(csv_config)
            #print csv_config
            for cc in csv_config:
                datasource_id = cc['datasource_id']
                csv_name = cc['csv_name']
                print csv_name
                qry=datasource_query_generate(datasource_id)
                csv_data[csv_name] = get_datasource_data_from_url(qry) #pd.read_sql(qry,connection)
                #print csv_data[csv_name]



   

    rows = filter(lambda sl: '' == sl['repeat'] , rpt_table)

    #start master table export

    if len(rows)>0:
        table_name=rows[0]["table_name"]
        table_cols = rows[0]["table_cols"]
        #print(table_cols)
        col_qry="id"
        for cols in table_cols:
            for k in cols.keys():
                col_qry += ", " +  cols[k] +" \"" + k + "\""
            #col=cols['diagnostic/rapid/species_recode_2']
            #col_alias = 
        #print col_qry
        with connection.cursor() as cursor:
            #print "select %s from core.bahis_%s_table" %(col_qry,table_name)
            cursor.execute("select %s from core.bahis_%s_table" %(col_qry,table_name)) 
            columns = [col[0] for col in cursor.description]
            r_ss_list= list(set(columns) & set(field_ss))
            r_ms_list= list(set(columns) & set(field_ms))
            ss_level={}
            ms_level={}

            for c_ss in  r_ss_list:
                df_ss_livel = df_field_list[df_field_list["field_name"]==c_ss]#["value_text","value_label"]
                df_ss_livel = df_ss_livel[["value_text","value_label"]]
                ss_level[c_ss] = df_ss_livel.set_index("value_text")["value_label"].to_dict()
                del df_ss_livel
            

            for c_ss in  r_ms_list:
                df_ms_livel = df_field_list[df_field_list["field_name"]==c_ss]#["value_text","value_label"]
                df_ms_livel = df_ms_livel[["value_text","value_label"]]
                ms_level[c_ss] = df_ms_livel.set_index("value_text")["value_label"].to_dict()
                del df_ms_livel


            #print columns
            db_gen = db_reader(cursor)
            #columns.sort()
            x_row=1
            x_col=1
            for c in columns:
                #ws[chr(col)+str(row)]=c
                ws.cell(row=x_row,column=x_col).value=c
                x_col +=1
            #wb.save(form_id_string + '.xlsx')
            wb.save(file_name)
            
            
            x_row +=1


            run_ms_done = False
            for row in db_gen:
                #print(row)
                r = dict(zip(columns, row))
                #print r
                #r_ss_list= list(set(r.keys()) & set(field_ss))
                for c_ss in  r_ss_list:
                    #print(c_ss + ':' + str(r[c_ss]))
                    #print c_ss
                    lookup = ss_level[c_ss]
                    #if c_ss =='basic_info/district':
                    if c_ss in csv_map:
                        #print lookup,r[c_ss]
                        csv_name = csv_map[c_ss]
                        #print csv_name
                        #lookup = json.loads(lookup)
                        l_key =  lookup.keys()[0]
                        l_val = json.loads(lookup[l_key])['English']
                        #print l_key,l_val
                        lookup_data = csv_data[csv_name]
                        lookup_data = lookup_data[lookup_data[l_key].astype(str)==r[c_ss]]
                        label = lookup_data.iloc[0][l_val] if len(lookup_data) >0 else str(r[c_ss])
                        r[c_ss]=label
                        #print label

                    #print lookup
                    try:
                        label = lookup[r[c_ss]]
                        label=json.loads(label)
                        r[c_ss]=label['English']
                        #print "-----------------------"
                        #print label
                    except:
                        label=r[c_ss]
                        pass

                    if c_ss =='patient_info/sex':
                        #print label
                        pass

                if (not run_ms_done) and len(r_ms_list)>0:
                    ms_row=1
                    ms_col=1
                    #ws_ms = wb.create_sheet("patient_registry_ms",1)
                    ws_ms = wb.create_sheet(index= 0 ,title=form_id_string+"_ms")
                    
                    ws_ms.cell(row=ms_row,column=ms_col).value="data_id"
                    ms_col +=1
                    ws_ms.cell(row=ms_row,column=ms_col).value="Field name"
                    ms_col +=1
                    ws_ms.cell(row=ms_row,column=ms_col).value="Value"
                    ms_col +=1

                    wb.save(file_name)
                    ms_row +=1   

                    run_ms_done =True                 

                for c_ss in  r_ms_list:
                    lookup = ms_level[c_ss]
                    #print c_ss,r[c_ss]
                    ms_vals = '' if r[c_ss] is None else r[c_ss].strip()
                    for msval in ms_vals.split(' '):
                        if msval=='':
                            continue
                        ws_ms.cell(row=ms_row,column=1).value=r["id"]
                        ws_ms.cell(row=ms_row,column=2).value=c_ss

                        try:
                            label = lookup[msval]
                            label=json.loads(label)
                            msval=label['English']
                        except:
                            #label=r[c_ss]
                            pass



                        ws_ms.cell(row=ms_row,column=3).value=msval
                        ms_row +=1   




                x_col=1
                #print (r)
                for c in columns:
                    #print r[c]
                    ws.cell(row=x_row,column=x_col).value=r[c]
                    x_col +=1
                x_row +=1
                wb.save(file_name)
        #df1.to_excel(writer, sheet_name='Sheet1')
    #end master table export
    print "here"
    print table_name
    print "here"

    #Start of child table export    

    for i,row in rpt_list.iterrows():
        #print row['field_name']
        rows = filter(lambda sl: row['field_name'] == sl['repeat'] , rpt_table)
        if len(rows)>0:
            #rows=rows[0]["table_name"]
            table_name=rows[0]["table_name"]
            table_cols = rows[0]["table_cols"]
            print table_name
            #print table_cols

            col_qry="id"
            for cols in table_cols:
                for k in cols.keys():
                    col_qry += ", " +  cols[k] +" \"" + k + "\""
                #col=cols['diagnostic/rapid/species_recode_2']
                #col_alias = 
            #print col_qry
            with connection.cursor() as cursor:
                print "select %s from core.bahis_%s_table" %(col_qry,table_name)
                cursor.execute("select %s from core.bahis_%s_table" %(col_qry,table_name)) 
                columns = [col[0] for col in cursor.description]

                r_ss_list = []
                r_ms_list = []

                r_ss_list= list(set(columns) & set(field_ss))
                r_ms_list= list(set(columns) & set(field_ms))
                ss_level={}
                ms_level={}

                for c_ss in  r_ss_list:
                    df_ss_livel = df_field_list[df_field_list["field_name"]==c_ss]#["value_text","value_label"]
                    df_ss_livel = df_ss_livel[["value_text","value_label"]]
                    ss_level[c_ss] = df_ss_livel.set_index("value_text")["value_label"].to_dict()
                    del df_ss_livel
                

                for c_ss in  r_ms_list:
                    df_ms_livel = df_field_list[df_field_list["field_name"]==c_ss]#["value_text","value_label"]
                    df_ms_livel = df_ms_livel[["value_text","value_label"]]
                    ms_level[c_ss] = df_ms_livel.set_index("value_text")["value_label"].to_dict()
                    del df_ms_livel

                db_gen = db_reader(cursor)
                #columns.sort()
                x_row=1
                x_col=1
                ws = wb.create_sheet(index= 0 ,title=table_name)
                for c in columns:
                    #ws[chr(col)+str(row)]=c
                    ws.cell(row=x_row,column=x_col).value=c
                    x_col +=1
                wb.save(file_name)
                x_row +=1


                run_ms_done = False
                for row in db_gen:
                    #print(row)
                    r = dict(zip(columns, row))
                    #print r
                    #r_ss_list= list(set(r.keys()) & set(field_ss))
                    for c_ss in  r_ss_list:
                        #print(c_ss + ':' + str(r[c_ss]))
                        #print c_ss
                        lookup = ss_level[c_ss]
                        #if c_ss =='basic_info/district':
                        if c_ss in csv_map:
                            #print lookup,r[c_ss]
                            csv_name = csv_map[c_ss]
                            #print csv_name
                            #lookup = json.loads(lookup)
                            l_key =  lookup.keys()[0]
                            l_val = json.loads(lookup[l_key])['English']
                            #print l_key,l_val
                            lookup_data = csv_data[csv_name]
                            lookup_data = lookup_data[lookup_data[l_key].astype(str)==r[c_ss]]
                            label = lookup_data.iloc[0][l_val] if len(lookup_data) >0 else str(r[c_ss])
                            r[c_ss]=label
                            #print label

                        else:
                            try:
                                label = lookup[r[c_ss]]
                                label=json.loads(label)
                                r[c_ss]=label['English']
                                #print "-----------------------"
                                #print label
                            except:
                                label=r[c_ss]
                                pass

                    if (not run_ms_done) and len(r_ms_list)>0:
                        ms_row=1
                        ms_col=1
                        #ws_ms = wb.create_sheet("patient_registry_ms",1)
                        s_title = form_id_string + "_ms"
                        s_title = s_title[:30]
                        ws_ms = wb.create_sheet(index=0, title=s_title)
                        
                        ws_ms.cell(row=ms_row,column=ms_col).value="data_id"
                        ms_col +=1
                        ws_ms.cell(row=ms_row,column=ms_col).value="Field name"
                        ms_col +=1
                        ws_ms.cell(row=ms_row,column=ms_col).value="Value"
                        ms_col +=1

                        wb.save(file_name)
                        ms_row +=1   

                        run_ms_done =True                 

                    for c_ss in  r_ms_list:
                        lookup = ms_level[c_ss]
                        #print c_ss,r[c_ss]
                        ms_vals = '' if r[c_ss] is None else r[c_ss].strip()
                        for msval in ms_vals.split(' '):
                            if msval=='':
                                continue
                            ws_ms.cell(row=ms_row,column=1).value=r["id"]
                            ws_ms.cell(row=ms_row,column=2).value=c_ss

                            try:
                                label = lookup[msval]
                                label=json.loads(label)
                                msval=label['English']
                            except:
                                #label=r[c_ss]
                                pass



                            ws_ms.cell(row=ms_row,column=3).value=msval
                            ms_row +=1   




                    x_col=1
                    #print (r)
                    for c in columns:
                        #print r[c]
                        ws.cell(row=x_row,column=x_col).value=r[c]
                        x_col +=1
                    x_row +=1
                    wb.save(file_name)

    Export.objects.filter(pk=export_id).update(filename=f_name, filedir=file_dir.replace('onadata/',''), internal_status=1)
                
    print Export.objects.filter(pk=export_id)

'''
@shared_task
def new_create_async_export(lan, xform, query, edt, id_string, export_type):
    print lan, xform, query, edt, id_string, export_type
    languages = []
    @transaction.atomic
    def _create_export(xform, export_type):
        return Export.objects.create(xform=xform, export_type=export_type)
    print "What should we do"
    print query

    export = _create_export(xform, export_type)
    username = xform.user.username
    df_form = []
    frm_id = xform.id
    client = MongoClient('192.168.19.88', 27017)
    conn = psycopg2.connect("dbname='mjivita' user='kobo' host='192.168.19.88' password='DB@mPower@786'")
    cur = conn.cursor()

    form_sql = "SELECT field_name, field_label, field_type, parent, value_text, value_label,  field_detail,sl_no,choice_filter FROM xform_extracted where xform_id=" + str(
        frm_id)
    print form_sql
    df_form = pd.read_sql(form_sql, conn)
    print df_form

    languages = get_languagelist(cur, frm_id)
    print("languages@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@")
    print(languages)
    db = client.mjivita
    collection = db.instances
    #print query
    mongo_data = collection.find(json.loads(query))
    #mongo_data = collection.find({"$and" : [ {"_xform_id_string":"water_management"},{"_submission_time":{"$gte":"2016-02-06T00:00:00","$lte":"2018-03-06T23:59:59"}},{"reporting_month":{"$regex": "2018-12"}},{"_submitted_by": { "$in" : ["bg","kbg","sbg","pbg","satbg","bg"] } } ] })
    export_excel(mongo_data, edt, lan, id_string, export.id, username, conn,languages,df_form)
'''

def get_languagelist(cur, frm_id):
    sql_lan="SELECT field_label FROM xform_extracted where xform_id="+str(frm_id)+" and field_label is not null and field_type not in('calculate') limit 1"
    cur.execute(sql_lan)
    rs=cur.fetchone()
    lan = rs[0]
    #print lan
    languages=[]
    try:
        json_object = json.loads(lan)
        for k,v in json_object.items():
            languages.append(k)
    except ValueError, e:
        languages.append("default")
    return languages


def get_field_type(fd, k):
    t = "NA"
    df_type = fd.loc[fd['field_name'] == k]['field_type'].head(1)
    if len(df_type) > 0:
        t = df_type.iloc[0]
    return t


def get_value_level(fd, k, v, lan, row,instance,languages,field_parent,common_list_level,common_list):
    #print (fd, k, v, lan, row,instance,languages,field_parent,common_list_level,common_list)
    #print (fd)
    lvl = v

    if k in common_list_level:        
        lvl=common_list_level[k]
        return lvl


    df_lvl=fd.loc[(fd['field_name']==k) & (fd['value_text']==v)][['value_label','choice_filter','field_detail']]
    #print k,v
    #print(df_lvl['field_detail'])
    #print(df_lvl['value_label'])
    #print(df_lvl['choice_filter'])
    if len(df_lvl)>0:
        pv=""
        if df_lvl['choice_filter'].iloc[0] is not None:
            parent_field=str(df_lvl['choice_filter'].iloc[0])
            #pv = common_list[parent_field]
            '''
            if k=='district':
                pv=common_list[parent_field]
            if k=='name_wma':
                pv=common_list[parent_field]
            if k=='name_wmg':
                pv=common_list[parent_field]
            if k=='polder':
                pv=common_list[parent_field]
            if k=='union':
                pv=common_list[parent_field]
            if k=='upazila':
                pv=common_list[parent_field]
            if k=='WMA_name':
                pv=common_list[parent_field]



            if k=='district' or k=='name_wma' or k=='name_wmg' or k=='polder' or k=='union' or k=='upazila' or k=='WMA_name':
                if parent_field in common_list:
                    pv=common_list[parent_field]
                else:
                    pv=""





            if field_parent[parent_field] == field_parent[k]:
                #print k,field_parent[k]
                c_index=row['_index']
                #print "here----" + str(c_index)
            else:
                if row['_parent_index']==-1:
                    c_index=1
                else:
                    c_index=row['_parent_index']


            for item in instance[field_parent[parent_field]]:
                if item['_index']==c_index:
                    pv=item[parent_field]
                    break
               '''
            #print pv  
        
        if pv =="":
            #IF multiple language
            if len(languages)>1:
                print("lang 1")
                l=json.loads(df_lvl['value_label'].iloc[0])
                print(l)
                print l[lan]
                lvl=l[lan]
            else:
                print("lang 2")
                print (lan)
                lvl=df_lvl['value_label'].iloc[0]
        else:
            for ix, rw in df_lvl.iterrows():                
                if rw['field_detail']['myfilter']==pv:
                    #print (rw['value_label'])
                    if len(languages)>1:
                        l=json.loads(rw['value_label'])
                        lvl=l[lan]
                    else:
                        lvl=rw['value_label']
                    break
    else:
        lvl=v

    if k in common_list:        
        common_list_level[k]=lvl
        #print "insert"

    return lvl



def parse_instance(d, p, g, cnt, pcnt,sh_collection,idx_collection,instance,fld_types,fld_select_splitted,field_parent,df_form,common_list):
    k=1
    j=0
    if p in idx_collection:
        i=idx_collection[p]
    else:
        idx_collection[p]=1

    #if p in sh_collection:
    #    j=0
    #else:
    sh_collection[p]={}
    sh_collection[p]["_index"]=cnt
    sh_collection[p]["_parent_index"]=pcnt
    sh_collection[p]["_parent_table"]=g

    ##                           MJIVITA specific                            ##

    sh_collection[p]["data_id"]=common_list["data_id"]

    if "ben_name" in common_list:
        sh_collection[p]["ben_name"] = common_list["ben_name"]
        sh_collection[p]["age"] = common_list["age"]
        sh_collection[p]["husband_name"] = common_list["husband_name"]
        sh_collection[p]["status"] = common_list["status"]
        sh_collection[p]["nid"] = common_list["nid"]
        sh_collection[p]["bid"] = common_list["bid"]

    if 'tlpin' in common_list:

        sh_collection[p]["tlpin"] = common_list["tlpin"]

    ##                           MJIVITA specific  ended                          ##
    for key, value in d.items():
        if type(value) == list:
            #print k
            for e in value:
                if type(e) == dict:
                    if str(key) in idx_collection:
                        k=idx_collection[str(key)]
                    else:
                        idx_collection[str(key)]=1
                        k=1

                    parse_instance(e,str(key),p,k,cnt,sh_collection,idx_collection,instance,fld_types,fld_select_splitted,field_parent,df_form,common_list)
                    idx_collection[str(key)]=idx_collection[str(key)]+1
                else:
                    if p in sh_collection:
                        sh_collection[p][key]=e
                        field_parent[key]=p
                    else:
                        print 'Erro: Dictionary no found'
        elif type(value) == dict:
            if str(key) in idx_collection:
                k=idx_collection[str(key)]
            else:
                idx_collection[str(key)]=1
                k=1
            parse_instance(value,str(key),p,k,cnt,sh_collection,idx_collection,instance,fld_types,fld_select_splitted,field_parent,df_form,common_list)
            idx_collection[str(key)]=idx_collection[str(key)]+1
        else:
            #print key
            if key in fld_types:
                data_type = fld_types[key]
            else:
                data_type = get_field_type(df_form,key)
                fld_types[key]=data_type
            #print data_type

            if data_type=="select all that apply":
                ms = value.split(' ')
                for msv in ms:                    
                    sh_collection[p][key+"_"+str(msv)]=msv
                    if (key+"_"+str(msv)) in fld_types:
                        j=0
                    else:
                        fld_types[key+"_"+str(msv)]="select all splitted"
                        fld_select_splitted[key+"_"+str(msv)]=key
                        


            if key <>'_userform_id' and key <>'_bamboo_dataset_id':
                sh_collection[p][key]=value
                field_parent[key]=p

        
    fsh_collection=sh_collection[p]

    if p in instance:
        instance[p].append(fsh_collection)
    else:
        instance[p]=[]
        instance[p].append(fsh_collection)
    del sh_collection[p]
    return fsh_collection


def export_excel(data, level_export, lan, frm_id_string, export_id, username, conn,languages,df_form):
    sh_collection = {}
    idx_collection = {}
    instance = {}
    fld_types = {}
    fld_select_splitted = {}
    field_parent = {}
    df_col = {}
    print_collection = {}
    common_list_level={}
    common_list = {}
    meta_list=[u'_status', u'_submission_time', u'_submitted_by', u'_uuid', u'_xform_id_string', u'approve', u'formhub/uuid',u'_id', '_index', '_parent_index', '_parent_table',]
    form_col_sql = "select distinct field_name,field_label,field_type,sl_no from xform_extracted where xform_id=(select id from logger_xform where id_string = '" + str(
        frm_id_string) + "') and field_type not in ('repeat','group') order by sl_no"
    df_form_col = pd.read_sql_query(form_col_sql, conn)
    ordered_col_list = df_form_col['field_name'].values.tolist()
    print (ordered_col_list)
    i = 0
    tcnt=0
    #print data
    for post in data:
        tcnt=tcnt+1
        #print tcnt
        common_list.clear()
        common_list_level.clear()

        ##                           MJIVITA specific                            ##
        householdid = ''

        sectorid = ''
        if "sectorid" in post:

            sectorid = post['sectorid']

        if "householdid" in post:

            householdid = post['householdid']

        print ("householdid and sector id is"+sectorid+','+householdid)

        if sectorid and householdid:

            q = "select tlpin_code from tlpin  where id = (select tlpin_id from sector where sector_code = '"+str(sectorid)+"') limit 1 "

            cursor = connection.cursor()

            cursor.execute(q)

            fetchVal = cursor.fetchone()

            if fetchVal:

                tlpin = fetchVal[0]

            else:

                tlpin = ''

            common_list["tlpin"] = tlpin

        common_list["data_id"] = post["_id"]

        if "uid" in post:

            u_id = post['uid']

            q = "select coalesce(name,'') as ben_name, coalesce(age,'') age,coalesce(status,'') status,coalesce(husband_name,'') husband_name,coalesce(nid,'') nid,coalesce(bid,'') bid from   beneficiary where uid = '"+str(u_id)+"' limit 1"

            cursor = connection.cursor()

            cursor.execute(q)

            fetchVal = cursor.fetchall()

            cursor.close()

            for temp in fetchVal:
                common_list["ben_name"] = temp[0]
                common_list["age"] = temp[1]
                common_list["status"] =  temp[2]
                common_list["husband_name"] =  temp[3]
                common_list["nid"] =  temp[4]
                common_list["bid"] =  temp[5]

        ##                           MJIVITA specific  ended                          ##
        
        if frm_id_string in idx_collection:
            i=idx_collection[frm_id_string]
        else:            
            idx_collection[frm_id_string]=1
            i=idx_collection[frm_id_string]

        parse_instance(post,frm_id_string,"",i,-1,sh_collection,idx_collection,instance,fld_types,fld_select_splitted,field_parent,df_form,common_list)
        idx_collection[frm_id_string]=idx_collection[frm_id_string]+1

        #tmp_instance=instance
        tmp_instance = deepcopy(instance)

        #Append the data to dataframe
        for l,m in tmp_instance.items():
            #print l
            for v in tmp_instance[l]:
                #If question type is select
                if level_export==1:
                    for x,y in v.items():
                        if x in fld_types:
                            data_type = fld_types[x]
                        else:
                            data_type = get_field_type(df_form,x)
                            fld_types[x]=data_type
                        #print data_type
                        if data_type=="select one":
                            v[x]= get_value_level(df_form,x,y,lan,v,instance,languages,field_parent,common_list_level,common_list)
                        #elif data_type=="select all that apply":
                            #v[x]= get_value_level(df_form,x,y,lan,v,instance,languages,field_parent,common_list_level,common_list)                    
                        #    v[x]=y
                        elif data_type == "select all splitted":
                            #print x,y
                            #print fld_select_splitted[x]
                            v[x]= get_value_level(df_form,fld_select_splitted[x],y,lan,v,instance,languages,field_parent,common_list_level,common_list)
                        else:
                            v[x]=y
                #df=pd.DataFrame(v, index=[0])
                df=v
                #print v
                if l in df_col:
                    df_col[l].append(df)        
                else:
                    df_col[l]=[]
                    df_col[l].append(df)
        instance.clear()
        tmp_instance.clear()
    #print datetime.datetime.now()

    # Order the data list
    for key,val in df_col.items():
        if key == '_validation_status':
            continue
    	sh_data=pd.DataFrame(df_col[key])
    	notordered_col_list= sh_data.columns.tolist()
    	tmp_list=deepcopy(notordered_col_list)
    	col_list=[]
    	for c in ordered_col_list:    		
    		if c in notordered_col_list:
    			col_list.append(c)
    			tmp_list.remove(c)
        #meta data removed from  exported data
    	for c in meta_list:
    		if c in notordered_col_list:
    			col_list.append(c)
    			tmp_list.remove(c)
    	for c in tmp_list:
    		col_list.append(c)

    	tmp_sh_data = deepcopy(sh_data[col_list])
        print_collection[key]=tmp_sh_data

    #print print_collection
    filedir = ''
    filename = ''
    if print_collection:
        
        filedir = 'onadata/media/'+username + "/exports/" + frm_id_string + "/xls"
        
        if not os.path.isdir(filedir):
            os.makedirs (filedir)
        filename = frm_id_string + '_' + datetime.now().strftime("%Y_%m_%d_%H_%M_%S") + ".xls"

        file_name = 'onadata/media/' + username + '/exports/' + frm_id_string + '/xls/' + frm_id_string + '_' + datetime.now().strftime(
        "%Y_%m_%d_%H_%M_%S") + ".xls"
        
        writer = ExcelWriter(file_name,engine='xlwt')

        for key, val in print_collection.items():

            val.to_excel(writer, key[:30], index=False)

        writer.save()

        Export.objects.filter(pk=export_id).update(filename=filename, filedir=filedir, internal_status=1)
    else:
        print 'empty******************'

        filename = 'No Data Found'

        Export.objects.filter(pk=export_id).update(filename=filename, filedir=filedir, internal_status=0)
    

    #writer = ExcelWriter(file_path, engine='xlwt')
    #main_df.to_excel(writer, 'Beneficiary list', index=False)
    print export_id
    print filename
    print filedir
    


@task()
def create_db_export(username, id_string, export_id, query=None,
                     force_xlsx=False, group_delimiter='/',
                     split_select_multiples=True,
                     binary_select_multiples=False, show_label=False):
    # we re-query the db instead of passing model objects according to
    # http://docs.celeryproject.org/en/latest/userguide/tasks.html#state
    ext = 'txt' if not force_xlsx else 'txt'

    try:
        logger = create_db_export.get_logger()
        logger.info("Enter to get export_id")
        # print 'Enter to get export_id'
        export = Export.objects.get(id=export_id)
    except Export.DoesNotExist:
        # no export for this ID return None.
        return None

    # though export is not available when for has 0 submissions, we
    # catch this since it potentially stops celery
    try:
        gen_export = generate_export(
            Export.DB_EXPORT, ext, username, id_string, export_id, query,
            group_delimiter, split_select_multiples, binary_select_multiples, show_label)
    except (Exception, NoRecordsFoundError) as e:
        export.internal_status = Export.FAILED
        export.save()
        # mail admins
        details = {
            'export_id': export_id,
            'username': username,
            'id_string': id_string
        }
        report_exception("DB Export Exception: Export ID - "
                         "%(export_id)s, /%(username)s/%(id_string)s"
                         % details, e, sys.exc_info())
        # Raise for now to let celery know we failed
        # - doesnt seem to break celery`
        raise
    else:
        return gen_export.id


@task()
def create_xls_export(username, id_string, export_id, query=None,
                      force_xlsx=True, group_delimiter='/',
                      split_select_multiples=True,
                      binary_select_multiples=False, show_label=False, label_lang=None, default_xls_fields=None):
    # we re-query the db instead of passing model objects according to
    # http://docs.celeryproject.org/en/latest/userguide/tasks.html#state
    print default_xls_fields
    ext = 'xls' if not force_xlsx else 'xlsx'

    try:
        export = Export.objects.get(id=export_id)
    except Export.DoesNotExist:
        # no export for this ID return None.
        return None

    # though export is not available when for has 0 submissions, we
    # catch this since it potentially stops celery
    try:
        gen_export = generate_export(
            Export.XLS_EXPORT, ext, username, id_string, export_id, query,
            group_delimiter, split_select_multiples, binary_select_multiples, show_label, label_lang,
            default_xls_fields)
    except (Exception, NoRecordsFoundError) as e:
        export.internal_status = Export.FAILED
        export.save()
        # mail admins
        details = {
            'export_id': export_id,
            'username': username,
            'id_string': id_string
        }
        report_exception("XLS Export Exception: Export ID - "
                         "%(export_id)s, /%(username)s/%(id_string)s"
                         % details, e, sys.exc_info())
        # Raise for now to let celery know we failed
        # - doesnt seem to break celery`
        raise
    else:
        return gen_export.id


@task()
def create_csv_export(username, id_string, export_id, query=None,
                      group_delimiter='/', split_select_multiples=True,
                      binary_select_multiples=False):
    # we re-query the db instead of passing model objects according to
    # http://docs.celeryproject.org/en/latest/userguide/tasks.html#state
    export = Export.objects.get(id=export_id)
    try:
        # though export is not available when for has 0 submissions, we
        # catch this since it potentially stops celery
        gen_export = generate_export(
            Export.CSV_EXPORT, 'csv', username, id_string, export_id, query,
            group_delimiter, split_select_multiples, binary_select_multiples)
    except NoRecordsFoundError:
        # not much we can do but we don't want to report this as the user
        # should not even be on this page if the survey has no records
        export.internal_status = Export.FAILED
        export.save()
    except Exception as e:
        export.internal_status = Export.FAILED
        export.save()
        # mail admins
        details = {
            'export_id': export_id,
            'username': username,
            'id_string': id_string
        }
        report_exception("CSV Export Exception: Export ID - "
                         "%(export_id)s, /%(username)s/%(id_string)s"
                         % details, e, sys.exc_info())
        raise
    else:
        return gen_export.id


@task()
def create_kml_export(username, id_string, export_id, query=None):
    # we re-query the db instead of passing model objects according to
    # http://docs.celeryproject.org/en/latest/userguide/tasks.html#state

    export = Export.objects.get(id=export_id)
    try:
        # though export is not available when for has 0 submissions, we
        # catch this since it potentially stops celery
        gen_export = generate_kml_export(
            Export.KML_EXPORT, 'kml', username, id_string, export_id, query)
    except (Exception, NoRecordsFoundError) as e:
        export.internal_status = Export.FAILED
        export.save()
        # mail admins
        details = {
            'export_id': export_id,
            'username': username,
            'id_string': id_string
        }
        report_exception("KML Export Exception: Export ID - "
                         "%(export_id)s, /%(username)s/%(id_string)s"
                         % details, e, sys.exc_info())
        raise
    else:
        return gen_export.id


@task()
def create_zip_export(username, id_string, export_id, query=None):
    export = Export.objects.get(id=export_id)
    try:
        gen_export = generate_attachments_zip_export(
            Export.ZIP_EXPORT, 'zip', username, id_string, export_id, query)
    except (Exception, NoRecordsFoundError) as e:
        export.internal_status = Export.FAILED
        export.save()
        # mail admins
        details = {
            'export_id': export_id,
            'username': username,
            'id_string': id_string
        }
        report_exception("Zip Export Exception: Export ID - "
                         "%(export_id)s, /%(username)s/%(id_string)s"
                         % details, e)
        raise
    else:
        if not settings.TESTING_MODE:
            delete_export.apply_async(
                (), {'export_id': gen_export.id},
                countdown=settings.ZIP_EXPORT_COUNTDOWN)
        return gen_export.id


@task()
def create_csv_zip_export(username, id_string, export_id, query=None,
                          group_delimiter='/', split_select_multiples=True,
                          binary_select_multiples=False):
    export = Export.objects.get(id=export_id)
    try:
        # though export is not available when for has 0 submissions, we
        # catch this since it potentially stops celery
        gen_export = generate_export(
            Export.CSV_ZIP_EXPORT, 'zip', username, id_string, export_id,
            query, group_delimiter, split_select_multiples,
            binary_select_multiples)
    except (Exception, NoRecordsFoundError) as e:
        export.internal_status = Export.FAILED
        export.save()
        # mail admins
        details = {
            'export_id': export_id,
            'username': username,
            'id_string': id_string
        }
        report_exception("CSV ZIP Export Exception: Export ID - "
                         "%(export_id)s, /%(username)s/%(id_string)s"
                         % details, e, sys.exc_info())
        raise
    else:
        return gen_export.id


@task()
def create_sav_zip_export(username, id_string, export_id, query=None,
                          group_delimiter='/', split_select_multiples=True,
                          binary_select_multiples=False):
    export = Export.objects.get(id=export_id)
    try:
        # though export is not available when for has 0 submissions, we
        # catch this since it potentially stops celery
        gen_export = generate_export(
            Export.SAV_ZIP_EXPORT, 'zip', username, id_string, export_id,
            query, group_delimiter, split_select_multiples,
            binary_select_multiples
        )
    except (Exception, NoRecordsFoundError) as e:
        export.internal_status = Export.FAILED
        export.save()
        # mail admins
        details = {
            'export_id': export_id,
            'username': username,
            'id_string': id_string
        }
        report_exception("SAV ZIP Export Exception: Export ID - "
                         "%(export_id)s, /%(username)s/%(id_string)s"
                         % details, e, sys.exc_info())
        raise
    else:
        return gen_export.id


@task()
def create_external_export(username, id_string, export_id, query=None,
                           token=None, meta=None):
    export = Export.objects.get(id=export_id)
    try:
        # though export is not available when for has 0 submissions, we
        # catch this since it potentially stops celery
        gen_export = generate_external_export(
            Export.EXTERNAL_EXPORT, username,
            id_string, export_id, token, query, meta
        )
    except (Exception, NoRecordsFoundError, ConnectionError) as e:
        export.internal_status = Export.FAILED
        export.save()
        # mail admins
        details = {
            'export_id': export_id,
            'username': username,
            'id_string': id_string
        }
        report_exception("External Export Exception: Export ID - "
                         "%(export_id)s, /%(username)s/%(id_string)s"
                         % details, e, sys.exc_info())
        raise
    else:
        return gen_export.id


@task()
def delete_export(export_id):
    try:
        export = Export.objects.get(id=export_id)
    except Export.DoesNotExist:
        pass
    else:
        export.delete()
        return True
    return False


SYNC_MONGO_MANUAL_INSTRUCTIONS = """
To re-sync manually, ssh into the server and run:

python manage.py sync_mongo -r [username] [id_string]\
--settings='settings.local_settings'

To force complete delete and re-creation, use the -a option:

python manage.py sync_mongo -ra [username] [id_string]\
--settings='settings.local_settings'
"""

REMONGO_PATTERN = re.compile(r'Total # of records to remongo: -?[1-9]+',
                             re.IGNORECASE)


@task()
def email_mongo_sync_status():
    """Check the status of records in the mysql db versus mongodb, and, if
    necessary, invoke the command to re-sync the two databases, sending an
    email report to the admins of before and after, so that manual syncing (if
    necessary) can be done."""

    before_report = mongo_sync_status()
    if REMONGO_PATTERN.search(before_report):
        # synchronization is necessary
        after_report = mongo_sync_status(remongo=True)
    else:
        # no synchronization is needed
        after_report = "No synchronization needed"

    # send the before and after reports, along with instructions for
    # syncing manually, as an email to the administrators
    mail_admins("Mongo DB sync status",
                '\n\n'.join([before_report,
                             after_report,
                             SYNC_MONGO_MANUAL_INSTRUCTIONS]))
